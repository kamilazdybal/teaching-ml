{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb54494a-42cd-44fe-a1a7-355931a3bd75",
   "metadata": {},
   "source": [
    "# Coding the policy gradient method in PyTorch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aec94e1-2367-427a-bc41-4519fa01778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0038bb5-fd30-4878-85e3-79a0998f600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv:\n",
    "\n",
    "    def __init__(self, \n",
    "                 max_pos=5, \n",
    "                 max_steps=50):\n",
    "        \n",
    "        self.max_pos = max_pos\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.x = 0\n",
    "        self.t = 0\n",
    "        \n",
    "        return self._state()\n",
    "\n",
    "    def step(self, a):\n",
    "        \n",
    "        self.t += 1\n",
    "\n",
    "        if a == 0:\n",
    "            self.x += -1 \n",
    "        else:\n",
    "            self.x += 1 \n",
    "\n",
    "        # Clip the state:\n",
    "        self.x = max(-self.max_pos, min(self.max_pos, self.x))\n",
    "\n",
    "        done = False\n",
    "        \n",
    "        r = -0.01\n",
    "\n",
    "        # Termination criteria:\n",
    "        if self.x >= self.max_pos:\n",
    "            \n",
    "            r = 1.0\n",
    "            \n",
    "            done = True\n",
    "      \n",
    "        if self.t >= self.max_steps:\n",
    "            \n",
    "            done = True\n",
    "            \n",
    "        return self._state(), r, done\n",
    "\n",
    "    def _state(self):\n",
    "\n",
    "        state = torch.tensor([self.x / float(self.max_pos)], dtype=torch.float32)\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ee53fd-2f10-4445-894c-8e47ee0343c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e1dd19-559b-407c-84fb-118f0cd88904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 8), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 8), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        logits = self.net(x)\n",
    "        \n",
    "        return Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da6ff85-e3a3-4c3a-96f8-37be216cc0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82dbc6d6-c5ad-4552-9276-647bc762780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_τ(env, \n",
    "               policy, \n",
    "               γ=0.95):\n",
    "\n",
    "    # Reset the environment to an initial state, s_0:\n",
    "    state = env.reset()\n",
    "    \n",
    "    ln_π_list = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "\n",
    "    # Step in the environment until termination:\n",
    "    while not done:\n",
    "\n",
    "        # Get the output of π for the current state:\n",
    "        distribution = policy(state.unsqueeze(0))\n",
    "\n",
    "        # Sample action from the current π:\n",
    "        action = distribution.sample()\n",
    "\n",
    "        # Compute the ln of π for that action:\n",
    "        ln_π = distribution.log_prob(action)\n",
    "\n",
    "        # Take a step in the environment:\n",
    "        state, r, done = env.step(action.item())\n",
    "\n",
    "        # Save the current ln(π) and reward:\n",
    "        ln_π_list.append(ln_π)\n",
    "        rewards.append(r)\n",
    "\n",
    "    # Once τ has terminated: - - - - - - - - - - - - -\n",
    "\n",
    "    # Compute the total return from this τ:\n",
    "    R = sum([(γ ** t) * r for t, r in enumerate(rewards)])\n",
    "    R = torch.tensor(R, dtype=torch.float32)\n",
    "\n",
    "    # Compute the sum of ln(π) across the trajectory:\n",
    "    sum_ln_π = torch.stack(ln_π_list).sum()\n",
    "    \n",
    "    return sum_ln_π, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf61be32-d0e2-4a72-bb99-b50c729d5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "N = 10\n",
    "α = 1e-3\n",
    "γ = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1066bea-348c-4a5d-85d6-ceef9aa60097",
   "metadata": {},
   "outputs": [],
   "source": [
    "θ = policy.parameters()\n",
    "optimizer = torch.optim.Adam(θ, lr=α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ea18d9-1442-4d2b-9296-1b22c257ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1, n_episodes+1):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    # Monte Carlo estimate from N trajectories:\n",
    "    for _ in range(0, N):\n",
    "        \n",
    "        sum_ln_π, R = generate_τ(env, policy, γ)\n",
    "        \n",
    "        loss += -(R.detach() * sum_ln_π)\n",
    "\n",
    "    # Compute the arithmetic average:\n",
    "    loss /= N\n",
    "\n",
    "    # Backpropagate loss and update θ:\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe8f7a-52f3-4477-9ea0-f705acdea724",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
